---
title: "Data Science Capstone Project"
author: "MB"
date: "March 17, 2015"
output: html_document
---

This is the capstone project of the Coursera Data Science specilization,
which aim is a model to predict the most probable word after
a set of 2 or 3 words. The model calculates the most probable word
based on sentences taken from
- blogs
- news
- twitter

```{r echo=FALSE}
setwd ("~")
library (stringr)
library (grid)
library (gridExtra)
library (VennDiagram)
library (wordcloud)
```

## Filtering data
In this section, sentences from blogs are used to instruct the model.

First, all upper case characters are converted to lower case:
in fact, the model proposed an upper case word if it is the
first word of a sentence. In addition, names of persons or places
are not in the scope of this predictor,
which is aimed at predicting common frequent words.

Then, all english-specific compressions, such as isn't or gonna, are expanded.

Similarly, technology-related words, like e-mail or i-phone,
are converted to the compact for, like email and iphone.

Finally, numers are removed, still because we want to predict words.

```{r, echo=FALSE, cache=TRUE}
parse_text <- function (infile) {
  f <- file (infile, open="r")
  line <- readLines(f, n = -1, warn = FALSE)

  line <- tolower (line)

  line <- str_replace_all (line, "\\bit['’]s\\b", " it is ")
  line <- str_replace_all (line, "\\bim\\b", " i am ")
  line <- str_replace_all (line, "\\bdidnt\\b", " did not ")
  line <- str_replace_all (line, "\\bwhat['’]s\\b", " what is ")
  line <- str_replace_all (line, "\\bwhere['’]s\\b", " where is ")
  line <- str_replace_all (line, "\\bdont\\b", " do not ")
  line <- str_replace_all (line, "\\bdoesnt\\b", " does not ")
  line <- str_replace_all (line, "\\bgonna\\b", " going to ")
  line <- str_replace_all (line, "\\bwanna\\b", " want to ")
  line <- str_replace_all (line, "\\bgotta\\b", " have got to ")

  line <- str_replace_all (line, "['’]s\\b", " ")

  line <- str_replace_all (line, "['’]ve\\b", " have ")
  line <- str_replace_all (line, "['’]ll\\b", " will ")
  line <- str_replace_all (line, "['’]d\\b", " would ")
  line <- str_replace_all (line, "['’]m\\b", " am ")
  line <- str_replace_all (line, "['’]re\\b", " are ")
  line <- str_replace_all (line, "n['’]t\\b", " not ")

  line <- str_replace_all (line, "\\be-", " e")
  line <- str_replace_all (line, "\\bi-", " i")

  line <- str_replace_all (line, "\\W", " ")
  line <- str_replace_all (line, "_", " ")
  line <- str_replace_all (line, "\\s+", " ")

  line <- str_replace_all (line, "\\S*\\d+\\S*", " ")

  line <- str_replace_all (line, "\\b[qwertyuopsdfghjklzxcvbnm]\\b", " ")

  line <- str_replace_all (line, "^ +", "")
  line <- str_replace_all (line, " +$", "")
  line <- str_replace_all (line, "  +", " ")
  
  close (f)
  
  return (line)
}

fileNames <- c ("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")
line_b <- parse_text (fileNames[1])
line_n <- parse_text (fileNames[2])
line_t <- parse_text (fileNames[3])
total_sentence <- (length (line_b), length (line_n), length (line_t))
```

## Tokenization
Words are separated now by blanks,
so sentenses are tokenized into words using blanks are separators.

```{r, echo=FALSE, cache=TRUE}
resource <- c("blogs", "news", "twitter", "merged")
token_b <- strsplit(line_b," ")
token_n <- strsplit(line_n," ")
token_t <- strsplit(line_t," ")
total_token <- c(length (unlist (token_b)), length (unlist (token_n)), length (unlist (token_t)))
```

## Words frequency
The frequency of each word is calculated
and the 100 most frequent words are listed:

```{r, echo=FALSE, cache=TRUE}
counts_b <- sort (table (unlist (token_b)), decr=T)
counts_n <- sort (table (unlist (token_n)), decr=T)
counts_t <- sort (table (unlist (token_t)), decr=T)
unique_token <- c(dim (counts_b), dim (counts_n), dim (counts_t))
par(mfrow=c(1,3))
wordcloud (row.names (counts_b[1:100]), as.numeric (counts_b[1:100]), colors=rainbow (100))
title (main=resource[1])
wordcloud (row.names (counts_n[1:100]), as.numeric (counts_n[1:100]), colors=rainbow (100))
title (main=resource[2])
wordcloud (row.names (counts_t[1:100]), as.numeric (counts_t[1:100]), colors=rainbow (100))
title (main=resource[3])
par(mfrow=c(1,1))
```

Not surprisingly, they are all very common words.
Overall, the most frequent is 'the' and it is interesting
to note that the ranking of 'I' increase from 'news', 'blogs' to 'twitter',
reflecting an increase of personal content.
Consequently, the data set the model is trained on should come from
a context similar to the context where the predictor will be applied.

### Similar frequent words in the data sets
In the 3 data sets,
namely the sentences from (i) blogs, (ii) news and (iii) twitter,
the most frequent words are similar.
The top 10000 most frequent words from each of the 3 data sets and their
overlap are shown in the following Venn diagram:

```{r, echo=FALSE, cache=TRUE}
grid.newpage ()
grid.draw (venn.diagram (list (blogs=row.names (counts_b),
                               news=row.names (counts_n),
                               twitter=row.names (counts_t)), filename=NULL))
```

The overlap among the 3 data sets is more than 60% in the top 10000 words.

In the followings, the 3 data sets are merged into a single data set
that is analyzed and used for the word predicting model.

```{r, echo=FALSE, cache=TRUE}
line <- c (line_b, line_n, line_t)
total_sentence <- c (total_sentence, length (line))
token <- strsplit (line," ")
total_token <- c (total_token, length (unlist (token)))
counts <- sort (table (unlist (token)), decr=T)
unique_token <- c (unique_token, dim (counts))
```

The merged data set includes `r total_token[4]` total tokens in
`r unique_token[4]` unique words.

In the following table,
the total number of tokens in the 3 data sets are shown
as well as the number of unique tokens, independent from their frequency.

```{r, echo=FALSE}
grid.table (data.frame (
  lines = total_sentence,
  words = total_token,
  unique_words = unique_token,
  row.names = resource))
```

## English words
All words obtained after tokenizing are checked in an english dictionary
to make sure they are properly spelled english words.

```{r, echo=FALSE, cache=TRUE}
vocabul_http <- "http://www-01.sil.org/linguistics/wordlists/english/wordlist/wordsEn.txt"
vocabul <- tolower (as.character (read.table (vocabul_http)$V1))
vocabul <- c (vocabul, "i")
total_vocabul <- length (unlist (vocabul))
```

### Profanity filtering
Profanity filtering is implicit in limiting the model to words that are
present in the vocabulary, assuming of course that the vocabulary is fair...

### Words not in the vocabulary
The words not in the vocabulary can be non-english words,
misspelled words,
names of persons or places.
The most frequent words not in the vocabulry are shown here,
to estimate what would be the most relevant source of error
introduced by ignoring them or cutting them out of the predictor model.

```{r, echo=FALSE, cache=TRUE}
unknown <- sort (table (subset (unlist (token), !(unlist (token) %in% vocabul))), de=T)
head (unknown, n=100)
```

The most frequent words not in vocabulary are technology related,
acronyms and contracted words;
the following words not in vocabulary are either typos or
names of people or companies.

### Expanding the vocabulary
To minimize the error introduced by ignoring words
not in the vocabulary,
the latter is expanded by adding words that still are too frequent
to be ignored.

The word count-cutoff to include words in the vocabulry is
defined based on the following plot where words are sorted
from most (left) to least (right) frequent.

```{r}
cutoff <- 1500
plot (log (as.numeric (unknown)), xlim=c(0,5000), ylab="ln (counts)")
abline (h=log (cutoff), col="red")
```

The word count cutoff is set at `r cutoff`:
words with counts larger than `r cutoff` are included in the vocabulary.
The following words are the top most frequent words that are not
included in the expanded vocabulary
and consequently excluded from the predictor model:

```{r, echo=FALSE}
subset (unknown, unknown>(cutoff-100) & unknown<cutoff)
```


```{r, echo=FALSE, cache=TRUE}
words2add <- subset (unknown, unknown>cutoff)
unique_2add <- dim (words2add)
total_2add <- sum (words2add)
vocabul <- c (vocabul, rownames (words2add))
unknown <- table (subset (unlist (token), !(unlist (token) %in% vocabul)))
unique_notvocab2 <- dim (unknown)
total_notvocab2 <- sum (unknown)
goodtokens <- subset (unlist (token), (unlist (token) %in% vocabul))
total_goodtokens <- length (goodtokens)
counts1 <- sort (table (goodtokens), decr=T)
unique_goodwords <- dim (counts1)

my_hist <-hist (log2 (as.numeric (counts1)), labels=TRUE, xlab="word counts", main="Histogram of log2(word_counts)", axes=F)
bins <- length (my_hist$counts)
axis (side=1, at=c (1:bins)-0.5,
      labels=c (1," "," "," "," ",2^5," "," "," "," ",2^10," "," "," "," ",2^15," "," "," "," ",2^20))
```

## Final figures
The total number of tokens is `r total_token[4]` which are `r unique_token[4]` unique words.
The original english vocabulary included `r total_vocabul` unique words and
`r unique_2add` unique words are later included in the vocabulary,
to end up with a vocabulary including `r length (vocabul)` unique words.
Consequently, `r unique_notvocab2` unique words are finally discarded from the data sets.
In total, `r unique_goodwords` unique words are present in the language
and are represented in 
`r total_goodtokens` total tokens that are kept for the following analysis
and the word predicting model.
That means that `r unique_notvocab2` unique words present in the original data sets
and represented by `r total_notvocab2` total tokens are ignored;
they correspond to `r 100 * total_notvocab2 / total_token[4]` % of the
total number of tokens.

## Language coverage
Some words appear much more frequently than others,
meaning the 50% of language coverage can be reached
by much less than 50% of the unique words.

```{r, cache=TRUE}
cumsum_counts1 <- cumsum (counts)
plot (cumsum_counts1/total_goodtokens, xlab="num of unique words", ylab="language coverage")
abline (h=0.5, col="green")
abline (h=0.9, col="violet")
```

`r sum (cumsum_counts1/total_goodtokens < .5)` unique words,
which are
`r 100*sum (cumsum_counts1/total_goodtokens < .5)/unique_goodwords`
% of the total numer of unique words, cover 50% of the language;
similarly,
`r sum (cumsum_counts1/total_goodtokens < .9)` unique words,
which are
`r 100*sum (cumsum_counts1/total_goodtokens < .9)/unique_goodwords`
% of the total numer of unique words, cover 90% of the language.

If synonyms are collapsed into one word only,
the total number of words would decrease and a smaller number of
words would be required to cover the same amount of language.

